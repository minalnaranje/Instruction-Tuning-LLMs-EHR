{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tzX3WickHHgY",
        "outputId": "956d3435-b2eb-4323-df9e-04801a572aaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.44.0\n",
            "  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.10.0\n",
            "  Downloading peft-0.10.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (0.5.3)\n",
            "Collecting tokenizers<0.20,>=0.19 (from transformers==4.44.0)\n",
            "  Downloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.44.0) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft==0.10.0) (5.9.5)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.44.0) (2025.1.31)\n",
            "Downloading transformers-4.44.0-py3-none-any.whl (9.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m98.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.19.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m119.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, tokenizers, nvidia-cusolver-cu12, transformers, bitsandbytes, peft\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.51.3\n",
            "    Uninstalling transformers-4.51.3:\n",
            "      Successfully uninstalled transformers-4.51.3\n",
            "  Attempting uninstall: peft\n",
            "    Found existing installation: peft 0.14.0\n",
            "    Uninstalling peft-0.14.0:\n",
            "      Successfully uninstalled peft-0.14.0\n",
            "Successfully installed bitsandbytes-0.45.5 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 peft-0.10.0 tokenizers-0.19.1 transformers-4.44.0\n",
            "Cloning into 'llemr'...\n",
            "remote: Enumerating objects: 105, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (67/67), done.\u001b[K\n",
            "remote: Total 105 (delta 49), reused 93 (delta 37), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (105/105), 4.05 MiB | 17.82 MiB/s, done.\n",
            "Resolving deltas: 100% (49/49), done.\n"
          ]
        }
      ],
      "source": [
        "# Install requirements\n",
        "!pip install transformers==4.44.0 peft==0.10.0 torch accelerate bitsandbytes\n",
        "\n",
        "   # Clone repo and get sample data\n",
        "!git clone https://github.com/zzachw/llemr.git\n",
        "!cp llemr/sample_data/qa_event_subset.jsonl .\n",
        "!cp llemr/sample_data/qa_note_subset.jsonl ."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "import re\n",
        "\n",
        "def init_model(model_name):\n",
        "    print(\"Loading model and tokenizer...\")\n",
        "\n",
        "    # Load model without quantization\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        model_name,\n",
        "        padding_side=\"left\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Set pad token if not set\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "def format_prompt(question, patient_data):\n",
        "    # Organize patient data by event type\n",
        "    events_by_type = {}\n",
        "    for item in patient_data:\n",
        "        event_type = item['event_type']\n",
        "        if event_type not in events_by_type:\n",
        "            events_by_type[event_type] = []\n",
        "        events_by_type[event_type].append(item)\n",
        "\n",
        "    # Extract time information from the question if present\n",
        "    time_pattern = r'at (?:the )?(\\d+\\.?\\d*)[- ]?(?:hour|hours|hr|hrs)?'\n",
        "    time_match = re.search(time_pattern, question.lower())\n",
        "    target_time = float(time_match.group(1)) if time_match else None\n",
        "\n",
        "    # Function to extract key terms from a question\n",
        "    def extract_key_terms(text):\n",
        "        # Convert to lowercase and split into words\n",
        "        words = text.lower().split()\n",
        "        # Remove common words and punctuation\n",
        "        stop_words = {'what', 'which', 'where', 'when', 'how', 'was', 'were', 'did', 'does', 'do', 'the', 'a', 'an', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by'}\n",
        "        key_terms = [word.strip('.,?!') for word in words if word.strip('.,?!') not in stop_words]\n",
        "        return set(key_terms)\n",
        "\n",
        "    # Extract key terms from the user's question\n",
        "    question_terms = extract_key_terms(question)\n",
        "\n",
        "    # Find matching questions based on key terms and time\n",
        "    matching_items = []\n",
        "    for item in patient_data:\n",
        "        item_terms = extract_key_terms(item['q'])\n",
        "        # Calculate term overlap\n",
        "        overlap = len(question_terms.intersection(item_terms))\n",
        "        if overlap >= 2:  # If at least 2 key terms match\n",
        "            matching_items.append((overlap, item['q'], item['a']))  # Store score, question and answer as tuple\n",
        "\n",
        "    # Sort by overlap score (first element of tuple)\n",
        "    matching_items.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "    # Build context with organized information\n",
        "    context = \"Patient Information:\\n\"\n",
        "\n",
        "    # If we have matching items, prioritize them\n",
        "    if matching_items:\n",
        "        context += f\"\\nRELEVANT INFORMATION:\\n\"\n",
        "        # Include up to 2 most relevant matches\n",
        "        for _, q, a in matching_items[:2]:\n",
        "            context += f\"Q: {q}\\nA: {a}\\n\"\n",
        "        context += \"\\n\"\n",
        "\n",
        "    # Priority order for event types\n",
        "    priority_events = [\n",
        "        'patient_demographics',\n",
        "        'admission_info',\n",
        "        'diagnoses_icd',\n",
        "        'procedureevents',\n",
        "        'labevents',\n",
        "        'prescriptions',\n",
        "        'microbiologyevents',\n",
        "        'transfers'\n",
        "    ]\n",
        "\n",
        "    # Add information in priority order\n",
        "    for event_type in priority_events:\n",
        "        if event_type in events_by_type:\n",
        "            context += f\"\\n{event_type.upper()}:\\n\"\n",
        "            items = events_by_type[event_type]\n",
        "\n",
        "            # If there's a time in the question and this is a time-sensitive event type\n",
        "            if target_time is not None and event_type in ['labevents', 'procedureevents', 'prescriptions', 'microbiologyevents']:\n",
        "                # Include all items that match the specific time\n",
        "                time_relevant_items = []\n",
        "                for item in items:\n",
        "                    if str(target_time) in item['q']:\n",
        "                        time_relevant_items.append(item)\n",
        "\n",
        "                # If we found time-specific items, only show those\n",
        "                if time_relevant_items:\n",
        "                    items = time_relevant_items\n",
        "\n",
        "            # Show all relevant items for the event type\n",
        "            shown_items = set()  # Track items we've already shown\n",
        "            for item in items:\n",
        "                # Check if this item's question is not in the matching items\n",
        "                if item['q'] not in [m[1] for m in matching_items]:\n",
        "                    item_text = f\"- {item['q']} {item['a']}\\n\"\n",
        "                    if item_text not in shown_items:\n",
        "                        context += item_text\n",
        "                        shown_items.add(item_text)\n",
        "\n",
        "    # Create the full prompt with more explicit instructions\n",
        "    return f\"\"\"<|system|>You are a medical AI assistant. Answer questions based ONLY on the following patient information. If the specific information needed is not in the context, say \"I don't have enough information to answer this question.\" Be concise and direct. If the question asks about a specific time, only use information from that exact time. DO NOT repeat the context in your answer - instead, provide a clear and direct response to the question.</s>\n",
        "<|user|>\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide a direct answer without repeating the context:</s>\n",
        "<|assistant|>\"\"\"\n",
        "\n",
        "def main():\n",
        "    # Free up memory\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Initialize model and tokenizer - using a smaller model\n",
        "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "\n",
        "    model, tokenizer = init_model(model_name)\n",
        "    model.eval()\n",
        "\n",
        "    # Get the device where the model is located\n",
        "    device = next(model.parameters()).device\n",
        "    print(f\"Model is on device: {device}\")\n",
        "\n",
        "    # Set up generation config with stricter parameters\n",
        "    generation_config = GenerationConfig(\n",
        "        do_sample=True,\n",
        "        temperature=0.1,  # Even lower temperature for more focused responses\n",
        "        top_p=0.5,       # More restrictive top_p\n",
        "        max_new_tokens=50,  # Shorter responses\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        repetition_penalty=1.2  # Penalize repetition\n",
        "    )\n",
        "\n",
        "    # Load sample data\n",
        "    print(\"Loading sample data...\")\n",
        "    try:\n",
        "        with open(\"sample_data/qa_event_subset.jsonl\", \"r\") as f:\n",
        "            event_data = [json.loads(line) for line in f]\n",
        "    except FileNotFoundError:\n",
        "        try:\n",
        "            with open(\"qa_event_subset.jsonl\", \"r\") as f:\n",
        "                event_data = [json.loads(line) for line in f]\n",
        "        except FileNotFoundError:\n",
        "            print(\"Error: Could not find the data file in either location\")\n",
        "            return\n",
        "\n",
        "    # Get unique patient IDs\n",
        "    patient_ids = sorted(list(set(item['hadm_id'] for item in event_data)))\n",
        "    print(f\"\\nAvailable patient IDs: {patient_ids[:5]} ...\")\n",
        "\n",
        "    # Show example questions for first patient\n",
        "    print(f\"\\nExample questions for patient {patient_ids[0]}:\")\n",
        "    example_questions = [item for item in event_data if item['hadm_id'] == patient_ids[0]][:3]\n",
        "    for q in example_questions:\n",
        "        print(f\"- {q['q']}\")\n",
        "\n",
        "    while True:\n",
        "        patient_id = input(\"\\nEnter patient ID (or 'quit' to exit): \")\n",
        "        if patient_id.lower() == 'quit':\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            patient_id = int(patient_id)\n",
        "            patient_questions = [item for item in event_data if item['hadm_id'] == patient_id]\n",
        "            if not patient_questions:\n",
        "                print(f\"No questions found for patient {patient_id}\")\n",
        "                continue\n",
        "\n",
        "            print(f\"\\nFound {len(patient_questions)} questions for patient {patient_id}\")\n",
        "            question = input(\"Enter your question (or 'list' to see available questions): \")\n",
        "\n",
        "            if question.lower() == 'list':\n",
        "                # Organize questions by event type\n",
        "                questions_by_type = {}\n",
        "                for i, q in enumerate(patient_questions):\n",
        "                    event_type = q['event_type']\n",
        "                    if event_type not in questions_by_type:\n",
        "                        questions_by_type[event_type] = []\n",
        "                    questions_by_type[event_type].append((i+1, q))\n",
        "\n",
        "                # Display questions organized by type\n",
        "                for event_type, questions in questions_by_type.items():\n",
        "                    print(f\"\\n{event_type.upper()}:\")\n",
        "                    for idx, q in questions:\n",
        "                        print(f\"{idx}. Q: {q['q']}\\n   A: {q['a']}\\n\")\n",
        "                continue\n",
        "\n",
        "            # Format input with chat template and patient context\n",
        "            input_text = format_prompt(question, patient_questions)\n",
        "\n",
        "            # Generate response\n",
        "            inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
        "            # Move inputs to the same device as the model\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "            try:\n",
        "                with torch.no_grad():\n",
        "                    outputs = model.generate(\n",
        "                        **inputs,\n",
        "                        generation_config=generation_config\n",
        "                    )\n",
        "                # Move outputs back to CPU for decoding\n",
        "                outputs = outputs.cpu()\n",
        "                response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                # Clean up the response - get only the assistant's part\n",
        "                if \"<|assistant|>\" in response:\n",
        "                    response = response.split(\"<|assistant|>\")[-1].strip()\n",
        "\n",
        "                print(f\"\\nModel Answer: {response}\")\n",
        "\n",
        "                # Find if this exact question exists in the dataset\n",
        "                matching_qa = next((item for item in patient_questions if item['q'].lower() == question.lower()), None)\n",
        "                if matching_qa:\n",
        "                    print(f\"True Answer from dataset: {matching_qa['a']}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during generation: {str(e)}\")\n",
        "                response = \"Error: Could not generate response\"\n",
        "\n",
        "        except ValueError:\n",
        "            print(\"Please enter a valid patient ID\")\n",
        "\n",
        "        print(\"\\n\" + \"-\" * 80)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iB-bYvNuH6PZ",
        "outputId": "f9d6cb98-296c-4010-a8f6-5a05add3636b"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model and tokenizer...\n",
            "Model is on device: cuda:0\n",
            "Loading sample data...\n",
            "\n",
            "Available patient IDs: [20044587, 20199380, 20214994, 20291550, 20297618] ...\n",
            "\n",
            "Example questions for patient 20044587:\n",
            "- Which interventions were carried out within the initial 48 hours?\n",
            "- Can you identify the three primary diagnoses billed for the patient?\n",
            "- Can you confirm the patient's marital status?\n",
            "\n",
            "Found 6 questions for patient 20199380\n",
            "\n",
            "LABEVENTS:\n",
            "1. Q: Which Hematology tests were conducted on the blood sample at the 50.92-hour mark?\n",
            "   A: The blood specimen was analyzed for Hematocrit, Hemoglobin, MCH, MCHC, MCV, Platelet Count, RDW, Red Blood Cells, White Blood Cells, RDW-SD, INR(PT), PT, and PTT.\n",
            "\n",
            "\n",
            "PRESCRIPTIONS:\n",
            "2. Q: What dosage was in the most recent Fentanyl Citrate prescription given within the initial 48 hours?\n",
            "   A: The prescription contained a 100mcg/2mL Amp of Fentanyl Citrate.\n",
            "\n",
            "\n",
            "PROCEDUREEVENTS:\n",
            "3. Q: Which interventions were carried out on the second day?\n",
            "   A: A 20-gauge procedure was performed.\n",
            "\n",
            "\n",
            "MICROBIOLOGYEVENTS:\n",
            "4. Q: Was a microbiology test conducted on the urine specimen of the patient on day 4?\n",
            "   A: Affirmative.\n",
            "\n",
            "5. Q: Was there a microbiology test conducted on the urine specimen within the initial 12 hours?\n",
            "   A: Negative.\n",
            "\n",
            "\n",
            "PATIENT_DEMOGRAPHICS:\n",
            "6. Q: Can you provide details on the patient's insurance coverage?\n",
            "   A: The patient had a different insurance plan.\n",
            "\n",
            "\n",
            "Found 6 questions for patient 20199380\n",
            "\n",
            "Model Answer: Answer: The blood specimen was analyzed for Hematocrit (HC), Hemoglobin (Hb), MCH, MCHC, MCV, Platelet Count (PC), RDW, Red Blood C\n",
            "True Answer from dataset: The blood specimen was analyzed for Hematocrit, Hemoglobin, MCH, MCHC, MCV, Platelet Count, RDW, Red Blood Cells, White Blood Cells, RDW-SD, INR(PT), PT, and PTT.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Found 6 questions for patient 20291550\n",
            "\n",
            "DIAGNOSES_ICD:\n",
            "1. Q: Can you provide the three primary diagnoses billed for the patient?\n",
            "   A: The patient was diagnosed with an open femur shaft fracture, thoracic aorta injury, and unspecified acute lung edema.\n",
            "\n",
            "4. Q: What was the initial diagnosis documented for the patient?\n",
            "   A: The patient had an open fracture of the femur shaft.\n",
            "\n",
            "\n",
            "ADMISSION_INFO:\n",
            "2. Q: Where was the patient admitted from?\n",
            "   A: The patient was admitted from the emergency room.\n",
            "\n",
            "3. Q: Can you confirm the patient's admission status?\n",
            "   A: The patient was admitted through the emergency department.\n",
            "\n",
            "\n",
            "LABEVENTS:\n",
            "5. Q: How many instances of Blood Free Calcium Blood Gas measurement were conducted on the patient within the initial 12-hour period?\n",
            "   A: Once\n",
            "\n",
            "\n",
            "PATIENT_DEMOGRAPHICS:\n",
            "6. Q: Can you provide details about the patient's insurance coverage?\n",
            "   A: The patient had a different insurance plan.\n",
            "\n",
            "Please enter a valid patient ID\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Found 6 questions for patient 20291550\n",
            "\n",
            "Model Answer: Yes, I can provide information about the primary diagnoses for the patient. According to the given patient information, the patient was diagnosed with an open femur shaft fracture, thoracic aorta injury, and unspec\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}